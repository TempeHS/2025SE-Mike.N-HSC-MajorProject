{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f35f541",
   "metadata": {},
   "source": [
    "**Sprint 2: Data Wrangling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04266fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site_feature_names.npy', 'X_train_baseline.npz', 'baseline_logreg_submission.csv', 'y_train.npy', 'best_params_logreg_submission.csv', 'X_test_engineered.npz', 'site_dic.pkl', 'test_sessions.csv', 'X_train_engineered.npz', 'sample_submission.csv', 'X_test_baseline.npz', 'feature_selection_logreg_submission.csv', 'train_sessions.csv', 'best_params_oversampled_logreg_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy.sparse import hstack\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ba629",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "868e673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = ('../data')\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af8f10",
   "metadata": {},
   "source": [
    "**Basic Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523dd4b",
   "metadata": {},
   "source": [
    "Convert timestamps into pd.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4429839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list columns for easy access\n",
    "sites_cols = ['site%s' % i for i in range(1, 11)]\n",
    "times_cols = ['time%s' % i for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ded2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timestamps to pd.datetime\n",
    "train_df[times_cols] = train_df[times_cols].apply(pd.to_datetime)\n",
    "test_df[times_cols] = test_df[times_cols].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03426aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by = 'time1')\n",
    "test_df = test_df.sort_values(by = 'time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "297f7fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "session_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "site1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time1",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time2",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time3",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time4",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time5",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time6",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time7",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time8",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time9",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time10",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "1589e55f-52c3-496e-a7e0-365b3ea057c1",
       "rows": [
        [
         "65540",
         "21",
         "2014-05-01 17:14:03",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "64199",
         "23",
         "2014-05-02 07:52:08",
         "66.0",
         "2014-05-02 07:54:08",
         "63.0",
         "2014-05-02 07:54:08",
         "2626.0",
         "2014-05-02 07:55:09",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2268",
         "979",
         "2014-05-02 07:57:51",
         "73.0",
         "2014-05-02 07:59:34",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29734",
         "66",
         "2014-05-02 08:05:16",
         "69.0",
         "2014-05-02 08:05:17",
         "67.0",
         "2014-05-02 08:05:17",
         "70.0",
         "2014-05-02 08:05:17",
         "71.0",
         "2014-05-02 08:05:17",
         "68.0",
         "2014-05-02 08:05:17",
         "71.0",
         "2014-05-02 08:05:18",
         "70.0",
         "2014-05-02 08:05:18",
         "69.0",
         "2014-05-02 08:05:18",
         "67.0",
         "2014-05-02 08:05:18"
        ],
        [
         "77048",
         "167",
         "2014-05-02 08:05:32",
         "167.0",
         "2014-05-02 08:05:33",
         "359.0",
         "2014-05-02 08:05:34",
         "167.0",
         "2014-05-02 08:05:34",
         "167.0",
         "2014-05-02 08:05:35",
         "305.0",
         "2014-05-02 08:09:19",
         "306.0",
         "2014-05-02 08:09:20",
         "306.0",
         "2014-05-02 08:09:22",
         "979.0",
         "2014-05-02 08:09:54",
         "68.0",
         "2014-05-02 08:12:46"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65540</th>\n",
       "      <td>21</td>\n",
       "      <td>2014-05-01 17:14:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64199</th>\n",
       "      <td>23</td>\n",
       "      <td>2014-05-02 07:52:08</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2014-05-02 07:54:08</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2014-05-02 07:54:08</td>\n",
       "      <td>2626.0</td>\n",
       "      <td>2014-05-02 07:55:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>979</td>\n",
       "      <td>2014-05-02 07:57:51</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2014-05-02 07:59:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29734</th>\n",
       "      <td>66</td>\n",
       "      <td>2014-05-02 08:05:16</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77048</th>\n",
       "      <td>167</td>\n",
       "      <td>2014-05-02 08:05:32</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-02 08:05:33</td>\n",
       "      <td>359.0</td>\n",
       "      <td>2014-05-02 08:05:34</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-02 08:05:34</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-02 08:05:35</td>\n",
       "      <td>305.0</td>\n",
       "      <td>2014-05-02 08:09:19</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2014-05-02 08:09:20</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2014-05-02 08:09:22</td>\n",
       "      <td>979.0</td>\n",
       "      <td>2014-05-02 08:09:54</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-02 08:12:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "65540          21 2014-05-01 17:14:03    NaN                 NaT    NaN   \n",
       "64199          23 2014-05-02 07:52:08   66.0 2014-05-02 07:54:08   63.0   \n",
       "2268          979 2014-05-02 07:57:51   73.0 2014-05-02 07:59:34    NaN   \n",
       "29734          66 2014-05-02 08:05:16   69.0 2014-05-02 08:05:17   67.0   \n",
       "77048         167 2014-05-02 08:05:32  167.0 2014-05-02 08:05:33  359.0   \n",
       "\n",
       "                         time3   site4               time4  site5  \\\n",
       "session_id                                                          \n",
       "65540                      NaT     NaN                 NaT    NaN   \n",
       "64199      2014-05-02 07:54:08  2626.0 2014-05-02 07:55:09    NaN   \n",
       "2268                       NaT     NaN                 NaT    NaN   \n",
       "29734      2014-05-02 08:05:17    70.0 2014-05-02 08:05:17   71.0   \n",
       "77048      2014-05-02 08:05:34   167.0 2014-05-02 08:05:34  167.0   \n",
       "\n",
       "                         time5  site6               time6  site7  \\\n",
       "session_id                                                         \n",
       "65540                      NaT    NaN                 NaT    NaN   \n",
       "64199                      NaT    NaN                 NaT    NaN   \n",
       "2268                       NaT    NaN                 NaT    NaN   \n",
       "29734      2014-05-02 08:05:17   68.0 2014-05-02 08:05:17   71.0   \n",
       "77048      2014-05-02 08:05:35  305.0 2014-05-02 08:09:19  306.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "65540                      NaT    NaN                 NaT    NaN   \n",
       "64199                      NaT    NaN                 NaT    NaN   \n",
       "2268                       NaT    NaN                 NaT    NaN   \n",
       "29734      2014-05-02 08:05:18   70.0 2014-05-02 08:05:18   69.0   \n",
       "77048      2014-05-02 08:09:20  306.0 2014-05-02 08:09:22  979.0   \n",
       "\n",
       "                         time9  site10              time10  \n",
       "session_id                                                  \n",
       "65540                      NaT     NaN                 NaT  \n",
       "64199                      NaT     NaN                 NaT  \n",
       "2268                       NaT     NaN                 NaT  \n",
       "29734      2014-05-02 08:05:18    67.0 2014-05-02 08:05:18  \n",
       "77048      2014-05-02 08:09:54    68.0 2014-05-02 08:12:46  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fff17c",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea870365",
   "metadata": {},
   "source": [
    "Some features explored in the EDA showed significant differences between Alice and Intruder. Let's put them into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3744a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer that adds categorical and time-based features to each session.\n",
    "\n",
    "    Features added:\n",
    "    - period1: 1 if the session starts at 12-13h or 18-19h, else 0\n",
    "    - period2: 1 if the session starts at 16-18h, else 0\n",
    "    - period3: 1 if the session starts at 0-12h, 14-15h, or 19-24h, else 0\n",
    "    - peak_alice_months: 1 if the session starts in November, February, or March, else 0\n",
    "    - mon_tue: 1 if the session starts on Monday or Tuesday, else 0\n",
    "    - wed_sat_sun: 1 if the session starts on Wednesday, Saturday, or Sunday, else 0\n",
    "    - year: Numeric year value of the session start\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with the new features as columns.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        hour = X['time1'].apply(lambda ts: ts.hour)\n",
    "        period1 = ((hour >= 12) & (hour <= 13) | (hour >= 18) & (hour <= 19)).astype('int')\n",
    "        period2 = ((hour >= 16) & (hour <= 18)).astype('int')\n",
    "        period3 = ((hour <= 12) & (hour >= 0) | (hour >= 19) & (hour <= 24) | (hour >= 14) & (hour <= 15)).astype('int')\n",
    "        month = X['time1'].apply(lambda ts: ts.month)\n",
    "        peak_alice_months = ((month == 11) | (month == 2) | (month == 3)).astype('int')\n",
    "        weekday = X['time1'].apply(lambda ts: ts.weekday()).astype('int')\n",
    "        mon_tue = ((weekday == 0) | (weekday == 1)).astype('int')\n",
    "        wed_sat_sun = ((weekday == 2) | (weekday == 5) | (weekday == 6)).astype('int')\n",
    "        year = X['time1'].apply(lambda ts: ts.year).astype('int')\n",
    "        X = np.c_[period1.values, period2.values, period3.values, peak_alice_months.values, \n",
    "                    mon_tue.values, wed_sat_sun.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e71cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer that adds two features to each session:\n",
    "\n",
    "    - session_duration: The duration of the session in seconds, transformed by raising to the power of 0.2.\n",
    "      Calculated as the difference between the latest and earliest timestamps among all session times.\n",
    "    - start_month: The session's start month, encoded as (year * 100 + month) and scaled.\n",
    "    - start_week: The session's start week, encoded as (year * 100 + weekofyear) and scaled.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with session_duration and start_month as columns.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        times = ['time%s' % i for i in range(1, 11)]\n",
    "        start_month = X['time1'].apply(lambda t: 100 * t.year + t.month).to_numpy() / 1e5\n",
    "        start_week = X['time1'].apply(lambda t: 100 * t.year + t.isocalendar().week).to_numpy() / 1e5\n",
    "        session_duration = ((X[times].max(axis=1) - X[times].min(axis=1)).dt.total_seconds() ** 0.2).to_numpy().ravel()  # 1D array\n",
    "        X = np.c_[session_duration, start_month, start_week]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9254a2",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). \n",
    "Parameters:\n",
    "- ngram-range: the length of ngrams to use. Here we want sequences of 1 to 5 urls.\n",
    "- max_features: the maximum amount of features to include. Higher might overfit.\n",
    "- tokenizer: what pattern to split by. Here we want to include the \".\" in urls, so the split must be defined as a whitespace.\n",
    "\n",
    "We use this to correlate the frequency of visited sites to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c29c743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Yury Kashnitsky's notebook - Model validation in a competition\n",
    "PATH_TO_DATA = '../data'\n",
    "path_to_train = os.path.join(PATH_TO_DATA, 'train_sessions.csv')\n",
    "path_to_test = os.path.join(PATH_TO_DATA, 'test_sessions.csv')\n",
    "path_to_site_dict = os.path.join(PATH_TO_DATA, 'site_dic.pkl')\n",
    "vectorizer_params = dict(ngram_range=(1, 5), max_features=50000, tokenizer = lambda s: s.split())\n",
    "\n",
    "def prepare_sparse_features(path_to_train, path_to_test, path_to_site_dict,\n",
    "                           vectorizer_params):\n",
    "    times = ['time%s' % i for i in range(1, 11)]\n",
    "    train_df = pd.read_csv(path_to_train,\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "    test_df = pd.read_csv(path_to_test,\n",
    "                      index_col='session_id', parse_dates=times)\n",
    "\n",
    "    # Sort the data by time\n",
    "    train_df = train_df.sort_values(by='time1')\n",
    "    \n",
    "    # read site -> id mapping provided by competition organizers \n",
    "    with open(path_to_site_dict, 'rb') as f:\n",
    "        site2id = pickle.load(f)\n",
    "    # create an inverse id _> site mapping\n",
    "    id2site = {v:k for (k, v) in site2id.items()}\n",
    "    # we treat site with id 0 as \"unknown\"\n",
    "    id2site[0] = 'unknown'\n",
    "    \n",
    "    # Transform data into format which can be fed into TfidfVectorizer\n",
    "    # This time we prefer to represent sessions with site names, not site ids. \n",
    "    # It's less efficient but thus it'll be more convenient to interpret model weights.\n",
    "    sites = ['site%s' % i for i in range(1, 11)]\n",
    "    train_sessions = train_df[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    test_sessions = test_df[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "    # to be split into 'mail', 'google' and 'com')\n",
    "    vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "    X_train = vectorizer.fit_transform(train_sessions)\n",
    "    X_test = vectorizer.transform(test_sessions)\n",
    "    y_train = train_df['target'].astype('int').values\n",
    "    \n",
    "    # we'll need site visit times for further feature engineering\n",
    "    train_times, test_times = train_df[times], test_df[times]\n",
    "    \n",
    "    return X_train, X_test, y_train, vectorizer, train_times, test_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80162d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, vectorizer, train_times, test_times = prepare_sparse_features(\n",
    "    path_to_train, path_to_test, path_to_site_dict, vectorizer_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "970ec04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 50000) (82797, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5992699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.academia-assets.com', '0.docs.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com 0.docs.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com 0.docs.google.com 0.docs.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com 0.drive.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com 0.talkgadget.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com apis.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com docs.google.com',\n",
       "       '0.docs.google.com 0.docs.google.com docs.google.com 0.talkgadget.google.com'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d75787",
   "metadata": {},
   "source": [
    "We make 2 feature engineer pipelines:\n",
    "* feature_pipeline: Returns a 2D-array of engineered features from the given dataset.\n",
    "* scaled_pipeline: Returns a 2D-array of engineered features (with scaling) from the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "451827b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the pipelines\n",
    "feature_pipeline = Pipeline([('feature_engineering', AttributesAdder())])\n",
    "\n",
    "scaled_pipeline = Pipeline([\n",
    "    ('scaled_feature_adder', ScaledAttributesAdder()),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b834a",
   "metadata": {},
   "source": [
    "FeatureUnion performs the transformation processes in parallel, concatenating them together at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef7df13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vectorizer_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('feature_pipeline', feature_pipeline),\n",
    "    ('scaled_pipeline', scaled_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dfb85f",
   "metadata": {},
   "source": [
    "Apply the all preprocessing processes to the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef7df13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_train = no_vectorizer_pipeline.fit_transform(train_df)\n",
    "engineered_test = no_vectorizer_pipeline.transform(test_df)\n",
    "X_train_full = hstack([X_train, engineered_train])\n",
    "X_test_full = hstack([X_test, engineered_test])\n",
    "y_train = train_df[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067dfda3",
   "metadata": {},
   "source": [
    "We will make a pipeline without the vectorizer to analyse the other transformation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be475ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    \"period1\", \"period2\", \"period3\", \"peak_alice_months\", \"mon_tue\", \"wed_sat_sun\"\n",
    "]\n",
    "scaled_columns = [\n",
    "    \"session_duration\", \"start_month\", \"start_week\"\n",
    "]\n",
    "\n",
    "X_train_no_vectorizer = no_vectorizer_pipeline.fit_transform(train_df)\n",
    "X_test_no_vectorizer = no_vectorizer_pipeline.transform(test_df)\n",
    "\n",
    "X_train_no_tokenizer_df = pd.DataFrame(\n",
    "    X_train_no_vectorizer, \n",
    "    columns=feature_columns + scaled_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57b88d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "period1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "period2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "period3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "peak_alice_months",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mon_tue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wed_sat_sun",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "session_duration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "start_month",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "start_week",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ddec5ca1-eb82-43e2-a2d7-591cc346659a",
       "rows": [
        [
         "count",
         "253561.0",
         "253561.0",
         "253561.0",
         "253561.0",
         "253561.0",
         "253561.0",
         "253561.0",
         "253561.0",
         "253561.0"
        ],
        [
         "mean",
         "0.17908905549354986",
         "0.13987561178572414",
         "0.771183265565288",
         "0.605660965211527",
         "0.35167868875734043",
         "0.31196437938010974",
         "1.0518277656447479e-13",
         "-9.700871591527024e-09",
         "-7.050718355415991e-09"
        ],
        [
         "std",
         "0.3834276274659564",
         "0.34685861600705076",
         "0.4200718181524086",
         "0.4887092206624792",
         "0.47749522283455886",
         "0.4632962895320078",
         "1.0000019719181017",
         "1.0000019719158346",
         "1.000001971917634"
        ],
        [
         "min",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "-2.379875516659652",
         "-1.7444049739515186",
         "-2.6592138495972266"
        ],
        [
         "25%",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "-0.678693948178149",
         "-1.48531433172698",
         "-1.180984186004094"
        ],
        [
         "50%",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "-0.15142079953774107",
         "0.6345181955682188",
         "0.5687570484521268"
        ],
        [
         "75%",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.5873239986629052",
         "0.6580718903160943",
         "0.6894288577258012"
        ],
        [
         "max",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "2.781738935498922",
         "0.6816255850639698",
         "0.870436571634973"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period1</th>\n",
       "      <th>period2</th>\n",
       "      <th>period3</th>\n",
       "      <th>peak_alice_months</th>\n",
       "      <th>mon_tue</th>\n",
       "      <th>wed_sat_sun</th>\n",
       "      <th>session_duration</th>\n",
       "      <th>start_month</th>\n",
       "      <th>start_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>253561.000000</td>\n",
       "      <td>253561.000000</td>\n",
       "      <td>253561.000000</td>\n",
       "      <td>253561.000000</td>\n",
       "      <td>253561.000000</td>\n",
       "      <td>253561.000000</td>\n",
       "      <td>2.535610e+05</td>\n",
       "      <td>2.535610e+05</td>\n",
       "      <td>2.535610e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.179089</td>\n",
       "      <td>0.139876</td>\n",
       "      <td>0.771183</td>\n",
       "      <td>0.605661</td>\n",
       "      <td>0.351679</td>\n",
       "      <td>0.311964</td>\n",
       "      <td>1.051828e-13</td>\n",
       "      <td>-9.700872e-09</td>\n",
       "      <td>-7.050718e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.383428</td>\n",
       "      <td>0.346859</td>\n",
       "      <td>0.420072</td>\n",
       "      <td>0.488709</td>\n",
       "      <td>0.477495</td>\n",
       "      <td>0.463296</td>\n",
       "      <td>1.000002e+00</td>\n",
       "      <td>1.000002e+00</td>\n",
       "      <td>1.000002e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.379876e+00</td>\n",
       "      <td>-1.744405e+00</td>\n",
       "      <td>-2.659214e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.786939e-01</td>\n",
       "      <td>-1.485314e+00</td>\n",
       "      <td>-1.180984e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.514208e-01</td>\n",
       "      <td>6.345182e-01</td>\n",
       "      <td>5.687570e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.873240e-01</td>\n",
       "      <td>6.580719e-01</td>\n",
       "      <td>6.894289e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.781739e+00</td>\n",
       "      <td>6.816256e-01</td>\n",
       "      <td>8.704366e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             period1        period2        period3  peak_alice_months  \\\n",
       "count  253561.000000  253561.000000  253561.000000      253561.000000   \n",
       "mean        0.179089       0.139876       0.771183           0.605661   \n",
       "std         0.383428       0.346859       0.420072           0.488709   \n",
       "min         0.000000       0.000000       0.000000           0.000000   \n",
       "25%         0.000000       0.000000       1.000000           0.000000   \n",
       "50%         0.000000       0.000000       1.000000           1.000000   \n",
       "75%         0.000000       0.000000       1.000000           1.000000   \n",
       "max         1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "             mon_tue    wed_sat_sun  session_duration   start_month  \\\n",
       "count  253561.000000  253561.000000      2.535610e+05  2.535610e+05   \n",
       "mean        0.351679       0.311964      1.051828e-13 -9.700872e-09   \n",
       "std         0.477495       0.463296      1.000002e+00  1.000002e+00   \n",
       "min         0.000000       0.000000     -2.379876e+00 -1.744405e+00   \n",
       "25%         0.000000       0.000000     -6.786939e-01 -1.485314e+00   \n",
       "50%         0.000000       0.000000     -1.514208e-01  6.345182e-01   \n",
       "75%         1.000000       1.000000      5.873240e-01  6.580719e-01   \n",
       "max         1.000000       1.000000      2.781739e+00  6.816256e-01   \n",
       "\n",
       "         start_week  \n",
       "count  2.535610e+05  \n",
       "mean  -7.050718e-09  \n",
       "std    1.000002e+00  \n",
       "min   -2.659214e+00  \n",
       "25%   -1.180984e+00  \n",
       "50%    5.687570e-01  \n",
       "75%    6.894289e-01  \n",
       "max    8.704366e-01  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the features were added correctly\n",
    "X_train_no_tokenizer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268b3ac",
   "metadata": {},
   "source": [
    "The ranges of features are within the expected values. Tranformation processes were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "855b3f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in sample: 0\n",
      "Infs in sample: 0\n",
      "Sample rows:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Nonzero elements: 4112650\n",
      "Sparsity: 0.03%\n",
      "Sample min: 0.0\n",
      "Sample max: 0.9765530230425825\n"
     ]
    }
   ],
   "source": [
    "# The transformed dataset is very large. We sample a few rows to take a look.\n",
    "sample = X_train[:1000].toarray()\n",
    "print(\"NaNs in sample:\", np.isnan(sample).sum())\n",
    "print(\"Infs in sample:\", np.isinf(sample).sum())\n",
    "print(\"Sample rows:\\n\", sample)\n",
    "\n",
    "# Check sparsity\n",
    "print(\"Nonzero elements:\", X_train.nnz)\n",
    "print(\"Sparsity: {:.2f}%\".format(100 * X_train.nnz / (X_train.shape[0] * X_train.shape[1])))\n",
    "\n",
    "# Check min/max (Make sure everything is scaled)\n",
    "print(\"Sample min:\", sample.min())\n",
    "print(\"Sample max:\", sample.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4112056",
   "metadata": {},
   "source": [
    "The dataset looks good, without NaNs, infs or large outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3e1d9",
   "metadata": {},
   "source": [
    "We extract the transformed dataset into new files. The transformed dataset is very sparse, and therefore must be stored in .npz files, which are more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54009f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "sparse.save_npz('../data/X_train_baseline.npz', X_train)\n",
    "sparse.save_npz('../data/X_test_baseline.npz', X_test)\n",
    "sparse.save_npz('../data/X_train_engineered.npz', X_train_full)\n",
    "sparse.save_npz('../data/X_test_engineered.npz', X_test_full)\n",
    "np.save('../data/y_train.npy', y_train)\n",
    "np.save(\"../data/site_feature_names.npy\", vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
