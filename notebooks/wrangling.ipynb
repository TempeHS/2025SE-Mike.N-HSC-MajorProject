{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f35f541",
   "metadata": {},
   "source": [
    "**Sprint 2: Data Wrangling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04266fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site_dic.pkl', 'sample_submission.csv', 'test_sessions.csv', 'train_sessions.csv']\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ba629",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "868e673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = ('../data')\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af8f10",
   "metadata": {},
   "source": [
    "**Basic Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523dd4b",
   "metadata": {},
   "source": [
    "Convert timestamps into pd.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4429839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list columns for easy access\n",
    "sites_cols = ['site%s' % i for i in range(1, 11)]\n",
    "times_cols = ['time%s' % i for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ded2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timestamps to pd.datetime\n",
    "train_df[times_cols] = train_df[times_cols].apply(pd.to_datetime)\n",
    "test_df[times_cols] = test_df[times_cols].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c03426aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by = 'time1')\n",
    "test_df = test_df.sort_values(by = 'time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "297f7fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "session_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "site1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time1",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time2",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time3",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time4",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time5",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time6",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time7",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time8",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time9",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "site10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time10",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "f3dbae37-48a4-4e47-8f34-faf653dabfc3",
       "rows": [
        [
         "65540",
         "21",
         "2014-05-01 17:14:03",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "64199",
         "23",
         "2014-05-02 07:52:08",
         "66.0",
         "2014-05-02 07:54:08",
         "63.0",
         "2014-05-02 07:54:08",
         "2626.0",
         "2014-05-02 07:55:09",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2268",
         "979",
         "2014-05-02 07:57:51",
         "73.0",
         "2014-05-02 07:59:34",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29734",
         "66",
         "2014-05-02 08:05:16",
         "69.0",
         "2014-05-02 08:05:17",
         "67.0",
         "2014-05-02 08:05:17",
         "70.0",
         "2014-05-02 08:05:17",
         "71.0",
         "2014-05-02 08:05:17",
         "68.0",
         "2014-05-02 08:05:17",
         "71.0",
         "2014-05-02 08:05:18",
         "70.0",
         "2014-05-02 08:05:18",
         "69.0",
         "2014-05-02 08:05:18",
         "67.0",
         "2014-05-02 08:05:18"
        ],
        [
         "77048",
         "167",
         "2014-05-02 08:05:32",
         "167.0",
         "2014-05-02 08:05:33",
         "359.0",
         "2014-05-02 08:05:34",
         "167.0",
         "2014-05-02 08:05:34",
         "167.0",
         "2014-05-02 08:05:35",
         "305.0",
         "2014-05-02 08:09:19",
         "306.0",
         "2014-05-02 08:09:20",
         "306.0",
         "2014-05-02 08:09:22",
         "979.0",
         "2014-05-02 08:09:54",
         "68.0",
         "2014-05-02 08:12:46"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65540</th>\n",
       "      <td>21</td>\n",
       "      <td>2014-05-01 17:14:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64199</th>\n",
       "      <td>23</td>\n",
       "      <td>2014-05-02 07:52:08</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2014-05-02 07:54:08</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2014-05-02 07:54:08</td>\n",
       "      <td>2626.0</td>\n",
       "      <td>2014-05-02 07:55:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>979</td>\n",
       "      <td>2014-05-02 07:57:51</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2014-05-02 07:59:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29734</th>\n",
       "      <td>66</td>\n",
       "      <td>2014-05-02 08:05:16</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-02 08:05:17</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-02 08:05:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77048</th>\n",
       "      <td>167</td>\n",
       "      <td>2014-05-02 08:05:32</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-02 08:05:33</td>\n",
       "      <td>359.0</td>\n",
       "      <td>2014-05-02 08:05:34</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-02 08:05:34</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-02 08:05:35</td>\n",
       "      <td>305.0</td>\n",
       "      <td>2014-05-02 08:09:19</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2014-05-02 08:09:20</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2014-05-02 08:09:22</td>\n",
       "      <td>979.0</td>\n",
       "      <td>2014-05-02 08:09:54</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-02 08:12:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "65540          21 2014-05-01 17:14:03    NaN                 NaT    NaN   \n",
       "64199          23 2014-05-02 07:52:08   66.0 2014-05-02 07:54:08   63.0   \n",
       "2268          979 2014-05-02 07:57:51   73.0 2014-05-02 07:59:34    NaN   \n",
       "29734          66 2014-05-02 08:05:16   69.0 2014-05-02 08:05:17   67.0   \n",
       "77048         167 2014-05-02 08:05:32  167.0 2014-05-02 08:05:33  359.0   \n",
       "\n",
       "                         time3   site4               time4  site5  \\\n",
       "session_id                                                          \n",
       "65540                      NaT     NaN                 NaT    NaN   \n",
       "64199      2014-05-02 07:54:08  2626.0 2014-05-02 07:55:09    NaN   \n",
       "2268                       NaT     NaN                 NaT    NaN   \n",
       "29734      2014-05-02 08:05:17    70.0 2014-05-02 08:05:17   71.0   \n",
       "77048      2014-05-02 08:05:34   167.0 2014-05-02 08:05:34  167.0   \n",
       "\n",
       "                         time5  site6               time6  site7  \\\n",
       "session_id                                                         \n",
       "65540                      NaT    NaN                 NaT    NaN   \n",
       "64199                      NaT    NaN                 NaT    NaN   \n",
       "2268                       NaT    NaN                 NaT    NaN   \n",
       "29734      2014-05-02 08:05:17   68.0 2014-05-02 08:05:17   71.0   \n",
       "77048      2014-05-02 08:05:35  305.0 2014-05-02 08:09:19  306.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "65540                      NaT    NaN                 NaT    NaN   \n",
       "64199                      NaT    NaN                 NaT    NaN   \n",
       "2268                       NaT    NaN                 NaT    NaN   \n",
       "29734      2014-05-02 08:05:18   70.0 2014-05-02 08:05:18   69.0   \n",
       "77048      2014-05-02 08:09:20  306.0 2014-05-02 08:09:22  979.0   \n",
       "\n",
       "                         time9  site10              time10  \n",
       "session_id                                                  \n",
       "65540                      NaT     NaN                 NaT  \n",
       "64199                      NaT     NaN                 NaT  \n",
       "2268                       NaT     NaN                 NaT  \n",
       "29734      2014-05-02 08:05:18    67.0 2014-05-02 08:05:18  \n",
       "77048      2014-05-02 08:09:54    68.0 2014-05-02 08:12:46  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fff17c",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83b16646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Fill NaN with zero values.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        sites = ['site%s' % i for i in range(1, 11)]\n",
    "        return X[sites].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb4731",
   "metadata": {},
   "source": [
    "We combine the site id to a single string for the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfc9dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerPrep(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Prepare site URLs for tokenizer (CountVectorizer).\n",
    "    \"\"\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        X = X.values.tolist()\n",
    "        # Each row is combine into 1 string, with each value (siteID) seperated by 1 whitespace.\n",
    "        # Put these strings into 1 list and return it.\n",
    "        return [\" \".join([str(site) for site in row]) for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea870365",
   "metadata": {},
   "source": [
    "Some features explored in the EDA showed significant differences between Alice and Intruder. Let's put them into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3744a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Extracts and engineers session-based and temporal features from the input DataFrame.\n",
    "        Features added:\n",
    "        minutes -- Duration of the session in minutes.\n",
    "        start_month -- Encoded year and month of session start (YYYYMM).\n",
    "        year -- Year of session start.\n",
    "        month -- Month of session start.\n",
    "        start_week -- Encoded year and ISO week of session start (YYYYWW).\n",
    "        start_day -- Day of year of session start.\n",
    "        start_hour -- Hour of session start.\n",
    "        dow -- Day of week of session start (0=Monday).\n",
    "        is_weekend -- 1 if session starts on weekend, else 0.\n",
    "        work_hours -- 1 if session starts during typical work hours (9 a.m. to 5 p.m.) and during Alice's active days (Monday, Tuesday, Thursday and Friday), else 0.\n",
    "        avg_time_per_site -- Average time spent per unique site in the session (minutes).\n",
    "        unique_sites -- Number of unique sites visited in the session.\n",
    "        repeated_site_visits -- Number of repeated site visits in the session.\n",
    "        ----------------------------------------------------------------------\n",
    "        Returns:\n",
    "            np.ndarray: Array of engineered features (no original columns).\n",
    "    \"\"\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        # Get the min and max of the timestamps.\n",
    "        min_times = X[times_cols].min(axis=1)\n",
    "        max_times = X[times_cols].max(axis=1)\n",
    "        # Duration of the session in minutes\n",
    "        minutes = ((max_times - min_times).dt.total_seconds() / 60).round(2)\n",
    "        # Start month of the session\n",
    "        start_month = min_times.dt.year * 100 + min_times.dt.month\n",
    "        # Year of the session (Ordinal)\n",
    "        year_map = {2013: 0, 2014: 1, 2015: 2}\n",
    "        year = min_times.dt.year.map(year_map)\n",
    "        # Month of the session\n",
    "        month = min_times.dt.month\n",
    "        # Start week of the session\n",
    "        start_week = min_times.dt.year * 100 + min_times.dt.isocalendar().week\n",
    "        # Start day of the session\n",
    "        start_day = min_times.dt.dayofyear\n",
    "        # Start hour of the session\n",
    "        start_hour = min_times.dt.hour\n",
    "        # Day of week of the session\n",
    "        dow = min_times.dt.weekday\n",
    "        # Whether of not the session started on the weekend.\n",
    "        is_weekend = min_times.dt.weekday.isin([5, 6]).astype(int)\n",
    "        # Whether of not the session started on an active Alice's day and work hour.\n",
    "        work_hours = (\n",
    "            min_times.dt.weekday.isin([0, 1, 3, 4]) & #Monday, Tuesday, Thursday and Friday\n",
    "            (min_times.dt.hour >= 9) & (min_times.dt.hour <= 17) #9 a.m. to 5 p.m.\n",
    "        ).astype(int)\n",
    "\n",
    "        # Average time spent per site in 1 session\n",
    "        site_counts = X[sites_cols].nunique(axis=1) # Total number of sites \n",
    "        avg_time_per_site = minutes.div(site_counts.where(site_counts > 0, np.nan)).fillna(0) # Minute duration / number of sites (0 if site_counts = 0)\n",
    "        \n",
    "        # Number of unique sites visited in 1 session\n",
    "        unique_sites = X[sites_cols].nunique(axis=1).astype(int)\n",
    "\n",
    "        # Number of repeated site visits in 1 session\n",
    "        repeated_site_visits = len(sites_cols) - X[sites_cols].nunique(axis=1).astype(int)\n",
    "\n",
    "        # Scalling the larger features (YYYYMM format) to be within [0,1] interval.\n",
    "        scaled = MinMaxScaler().fit_transform(\n",
    "            np.c_[\n",
    "                start_month.values,\n",
    "                start_week.values\n",
    "            ]\n",
    "        )\n",
    "        start_month_scaled = scaled[:, 0]\n",
    "        start_week_scaled = scaled[:, 1]\n",
    "\n",
    "        # Return as numpy array (no DataFrame, no original columns)\n",
    "        return np.c_[\n",
    "            minutes.values,\n",
    "            start_month_scaled,\n",
    "            year.values,\n",
    "            month.values,\n",
    "            start_week_scaled,\n",
    "            start_day.values,\n",
    "            start_hour.values,\n",
    "            dow.values,\n",
    "            is_weekend.values,\n",
    "            work_hours.values,\n",
    "            avg_time_per_site.values,\n",
    "            unique_sites.values,\n",
    "            repeated_site_visits.values\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d75787",
   "metadata": {},
   "source": [
    "We make 2 pipelines:\n",
    "* vectorizer_pipeline: prepares the dataset for tokenizer by imputing NaNs in siteID columns, and combining them into a list of strings. CountVectorizer() converts a collection of text into a matrix of token counts. ngram_range of (1,3) means that the vectorizer will extract unigrams (single site IDs), bigrams (pairs of site IDs) and trigrams (triplets of site IDs). Only the 10000 most frequent n-grams are kept as features. Returns a a 2D-array of these features.\\\n",
    "For example, given array = ['00' , '01', '00', '02'], the CountVectorizer will return a matrix that counts up the frequency of each element (token): \\\n",
    "[['00', '01', '02'], \\\n",
    "[2, 1, 1]] \n",
    "* feature_pipeline: Returns a 2D-array of engineered features from the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "451827b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the pipelines\n",
    "vectorizer_pipeline = Pipeline([('tokenizer_prep', TokenizerPrep()), ('tokenizer', CountVectorizer(ngram_range=(1,4), max_features=10000))])\n",
    "\n",
    "feature_pipeline = Pipeline([('feature_engineering', FeatureAdder())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b834a",
   "metadata": {},
   "source": [
    "FeatureUnion performs the transformation processes in parallel, concatenating them together at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef7df13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing pipeline\n",
    "preprocessing = FeatureUnion(transformer_list=[('vectorizer', vectorizer_pipeline), ('feature_engineering', feature_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef7df13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing.fit_transform(train_df)\n",
    "X_test = preprocessing.transform(test_df)\n",
    "\n",
    "y_train = train_df[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "855b3f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in sample: 0\n",
      "Infs in sample: 0\n",
      "Sample rows:\n",
      " [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.00000000e+00 8.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.48850000e+01\n",
      "  2.00000000e+00 8.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.16666667e-02\n",
      "  6.00000000e+00 4.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.00000000e-02\n",
      "  5.00000000e+00 5.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.00000000e-03\n",
      "  6.00000000e+00 4.00000000e+00]]\n",
      "Nonzero elements: 13865139\n",
      "Sparsity: 0.55%\n",
      "Sample min: 0.0\n",
      "Sample max: 29.77\n"
     ]
    }
   ],
   "source": [
    "# The transformed dataset is very large. We sample a few rows to take a look.\n",
    "sample = X_train[:5].toarray()\n",
    "print(\"NaNs in sample:\", np.isnan(sample).sum())\n",
    "print(\"Infs in sample:\", np.isinf(sample).sum())\n",
    "print(\"Sample rows:\\n\", sample)\n",
    "\n",
    "# Check sparsity\n",
    "print(\"Nonzero elements:\", X_train.nnz)\n",
    "print(\"Sparsity: {:.2f}%\".format(100 * X_train.nnz / (X_train.shape[0] * X_train.shape[1])))\n",
    "\n",
    "# Check min/max (Make sure everything is scaled)\n",
    "print(\"Sample min:\", sample.min())\n",
    "print(\"Sample max:\", sample.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4112056",
   "metadata": {},
   "source": [
    "The dataset looks good, without NaNs, infs or large outliers, but is too large to be manually inspected. Let's run a simple model to see if the preprocessing steps done have improved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "762f0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbdc0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "my_model_1 = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdce4530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888619299099707\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(my_model_1, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084d91a",
   "metadata": {},
   "source": [
    "The cv_score is greater than 0.5 for a logistic regression problem, which suggest that the added features improved the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178bcb0",
   "metadata": {},
   "source": [
    "This will be the baseline for other models, which we will explore in the next phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
